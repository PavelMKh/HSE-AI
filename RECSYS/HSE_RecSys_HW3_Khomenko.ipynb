{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP3QpbyFWZCp"
      },
      "source": [
        "## ДЗ №3 Двухуровневый пайплайн\n",
        "#### В этой домашке вам предстоит написать с нуля двустадийную рекомендательную систему.\n",
        "\n",
        "#### Дата выдачи: 10.03.25\n",
        "\n",
        "#### Мягкий дедлайн: 31.03.25 23:59 MSK\n",
        "\n",
        "#### Жесткий дедлайн: 7.04.25 23:59 MSK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSlPlfnkWZCq"
      },
      "source": [
        "### Описание\n",
        "Это творческое задание, в котором вам необходимо реализовать полный цикл построения рекомендательной системы: реализовать кандидат генераторов, придумать и собрать признаки, обучить итоговый ранкер и заинференсить модели на всех пользователей.\n",
        "\n",
        "Вам предоставляется два набора данных: `train.csv` и `test.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cdzC4oEVWZCr"
      },
      "outputs": [],
      "source": [
        "# скачиваем данные\n",
        "# если из этой ячейки не получается, то вот ссылка на папку https://drive.google.com/drive/folders/1HT0Apm8Jft0VPLJtdBBUGu9s1M7vZcoJ?usp=drive_link\n",
        "\n",
        "# !pip3 install gdown\n",
        "\n",
        "\n",
        "# import gdown\n",
        "# # train\n",
        "# url = \"https://drive.google.com/file/d/1-CcS22-UpTJeNcFlA0dVLrEQn8jnI0d-/view?usp=drive_link\"\n",
        "# output = 'train.csv'\n",
        "# gdown.download(url, output, quiet=False)\n",
        "\n",
        "# # test\n",
        "# url = \"https://drive.google.com/file/d/11iz3xDh0IIoEIBY0dyRSvByY3qfiT3BG/view?usp=drive_link\"\n",
        "# output = 'test.csv'\n",
        "# gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install"
      ],
      "metadata": {
        "id": "-1sK7HXhYzN6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from typing import Union\n",
        "from scipy.sparse.linalg import svds\n",
        "from scipy.sparse import coo_matrix, coo_array, csr_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "metadata": {
        "id": "P_xxRVlgX6JU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt2QAsgMWZCr"
      },
      "source": [
        "\n",
        "\n",
        "### 1 Этап. Модели первого уровня. (max 3 балла)\n",
        "В этом этапе вам необходимо разделить `train` датасет на 2 части: для обучения моделей первого уровня и для их валидации. Единственное условие для разбиения – разбивать нужно по времени. Данные для обучение будем называть `train_stage_1`, данные для валидации `valid_stage_1`. Объемы этих датасетов вы определяет самостоятельно.\n",
        "\n",
        "Для начала нам нужно отобрать кандидатов при помощи легких моделей. Необходимо реализовать 3 типа моделей:\n",
        "1. Любая эвристическая(алгоритмичная) модель на ваш выбор **(0.5 балл)**\n",
        "2. Любая матричная факторизация на ваш выбор **(1 балл)**\n",
        "3. Любая нейросетевая модель на ваш выбор **(1 балла)**\n",
        "\n",
        "Не забудьте использовать скор каждой модели, как признак!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_val(sample_size: int = 10000):\n",
        "    df = pd.read_csv('train_part.csv')\n",
        "    df_valid = pd.read_csv('test_part.csv')\n",
        "\n",
        "    train_end = '2021-07-01'\n",
        "    df_train = df[df['last_watch_dt'] < train_end].copy()\n",
        "    df_test = df[df['last_watch_dt'] >= train_end].copy()\n",
        "\n",
        "    train_users = df_train['user_id'].unique()\n",
        "    train_items = df_train['item_id'].unique()\n",
        "\n",
        "    df_test = df_test[df_test['user_id'].isin(train_users)]\n",
        "    df_test = df_test[df_test['item_id'].isin(train_items)]\n",
        "\n",
        "    unique_user_ids = df_train['user_id'].unique()\n",
        "\n",
        "    selected_user_ids = np.random.choice(unique_user_ids, size=sample_size, replace=False)\n",
        "    df_train_sample = df_train[df_train['user_id'].isin(selected_user_ids)].copy()\n",
        "    df_test_sample = df_test[df_test['user_id'].isin(selected_user_ids)].copy()\n",
        "    df_valid_sample = df_valid[df_valid['user_id'].isin(selected_user_ids)].copy()\n",
        "\n",
        "    all_user_ids = np.unique(np.concatenate([df_train_sample['user_id'], df_test_sample['user_id'], df_valid_sample['user_id']]))\n",
        "    all_item_ids = np.unique(np.concatenate([df_train_sample['item_id'], df_test_sample['item_id'], df_valid_sample['item_id']]))\n",
        "\n",
        "    user_le = LabelEncoder()\n",
        "    item_le = LabelEncoder()\n",
        "\n",
        "    user_le.fit(all_user_ids)\n",
        "    item_le.fit(all_item_ids)\n",
        "\n",
        "    df_train_sample['user_id'] = user_le.transform(df_train_sample['user_id'])\n",
        "    df_train_sample['item_id'] = item_le.transform(df_train_sample['item_id'])\n",
        "\n",
        "    df_test_sample['user_id'] = user_le.transform(df_test_sample['user_id'])\n",
        "    df_test_sample['item_id'] = item_le.transform(df_test_sample['item_id'])\n",
        "\n",
        "    df_valid_sample['user_id'] = user_le.transform(df_valid_sample['user_id'])\n",
        "    df_valid_sample['item_id'] = item_le.transform(df_valid_sample['item_id'])\n",
        "\n",
        "    return df_train_sample, df_test_sample, df_valid_sample"
      ],
      "metadata": {
        "id": "jOKgIwElEY2k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test, df_valid = get_train_test_val()\n",
        "df_train.shape, df_test.shape, df_valid.shape"
      ],
      "metadata": {
        "id": "mp2hezBCEZo0",
        "outputId": "e9d79c60-97a1-4e8d-e10f-e941ee1628a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47575, 6), (16428, 6), (3480, 6))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2. Модель матричной факторизации (SVD)"
      ],
      "metadata": {
        "id": "GBpZgKi0Tf9k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIgaWPh4WZCs"
      },
      "source": [
        "Каждая модель должна уметь:\n",
        "1) для пары user_item предсказывать скор релевантности (масштаб скора не важен), важно обработать случаи, когда модель не можеn проскорить пользователя или айтем, вместо этого вернуть какое-то дефолтное значение\n",
        "2) для всех пользователей вернуть top-k самых релевантных айтемов (тут вам скоры не нужны)\n",
        "\n",
        "\n",
        "Дополнительно можно провести анализ кандидат генератов, измерить насколько различные айтемы они рекомендуют, например с помощью таких метрик как: [Ranked based overlap](https://github.com/changyaochen/rbo) или различные вариации [Diversity](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/master/Base/Evaluation/metrics.py#L289). **(1 балл)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SVD_factorization():\n",
        "    def __init__(self, n_singular_values: int = -1) -> None:\n",
        "        self.n_singular_values = n_singular_values\n",
        "        self.recs = None\n",
        "        self.user_features = None\n",
        "        self.item_features = None\n",
        "\n",
        "    def _df_to_matrix(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        interaction_matrix = df.pivot_table(index='user_id', columns='item_id', values='target', fill_value=0)\n",
        "        result = interaction_matrix.values\n",
        "        return result\n",
        "\n",
        "    def _make_svd(self, interactions: np.ndarray):\n",
        "        U, S, Vt = np.linalg.svd(interactions, full_matrices=False)\n",
        "\n",
        "        if self.n_singular_values != -1:\n",
        "            U = U[:, :self.n_singular_values]\n",
        "            S = S[:self.n_singular_values]\n",
        "            Vt = Vt[:self.n_singular_values, :]\n",
        "\n",
        "        self.user_features = U\n",
        "        self.item_features = Vt.T\n",
        "        return U, S, Vt\n",
        "\n",
        "    def fit(self, df_train: pd.DataFrame):\n",
        "        interactions = self._df_to_matrix(df_train)\n",
        "        U, S, Vt = self._make_svd(interactions)\n",
        "\n",
        "        self._calculate_recommendations(interactions)\n",
        "\n",
        "    def _calculate_recommendations(self, interactions):\n",
        "        n_users, n_items = interactions.shape\n",
        "        relevance_scores = np.zeros((n_users, n_items))\n",
        "\n",
        "        for user in range(n_users):\n",
        "            interacted_items = np.where(interactions[user] > 0)[0]\n",
        "\n",
        "            user_features = self.user_features[user]\n",
        "            item_features = self.item_features\n",
        "\n",
        "            relevance_scores[user] = np.dot(user_features, item_features.T)\n",
        "\n",
        "            if np.isnan(relevance_scores[user]).any():\n",
        "                relevance_scores[user] = np.nan_to_num(relevance_scores[user], nan=0.0)\n",
        "\n",
        "            relevance_scores[user, interacted_items] = -np.inf\n",
        "\n",
        "        self.recs = relevance_scores\n",
        "\n",
        "    def predict_relevance(self, user_id: int, item_id: int) -> float:\n",
        "        if self.user_features is None or self.item_features is None:\n",
        "            raise ValueError(\"Model is not fitted yet.\")\n",
        "\n",
        "        user_features = self.user_features[user_id]\n",
        "        item_features = self.item_features[item_id]\n",
        "\n",
        "        relevance_score = np.dot(user_features, item_features)\n",
        "\n",
        "        if np.isnan(relevance_score):\n",
        "            return 0.0\n",
        "\n",
        "        return relevance_score\n",
        "\n",
        "    def get_top_k_recommendations(self, user_id: int, top_k: int) -> list:\n",
        "        if self.recs is None:\n",
        "            raise ValueError(\"Recommendations are not calculated yet.\")\n",
        "\n",
        "        relevance_scores = self.recs[user_id]\n",
        "        top_k_indices = np.argsort(relevance_scores)[-top_k:][::-1]\n",
        "\n",
        "        return top_k_indices.tolist()"
      ],
      "metadata": {
        "id": "2Ufw0PhHSJVF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mf = SVD_factorization()\n",
        "\n",
        "mf.fit(df_train)"
      ],
      "metadata": {
        "id": "U1W9l3P8SU_J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mf.get_top_k_recommendations(user_id=4631, top_k=10)"
      ],
      "metadata": {
        "id": "JpKpSLuhYHjL",
        "outputId": "55776235-f70a-4e39-8be7-86a6916812ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4345, 4651, 4647, 4605, 4600, 4595, 4632, 4446, 4582, 4452]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mf.predict_relevance(4631, 3739)"
      ],
      "metadata": {
        "id": "pBBEmdapZ2aw",
        "outputId": "83a1a731-9299-4dc2-a3fa-c43bd88709f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0027532637826253297"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grphavZ7WZCr"
      },
      "outputs": [],
      "source": [
        "my_heuristic_model = # YOUR CODE HERE\n",
        "my_matrix_factorization = # YOUR CODE HERE\n",
        "my_neural_network = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTeoyEtiWZCs"
      },
      "source": [
        "\n",
        "### 2 Этап. Генерация и сборка признаков. (max 2 балла)\n",
        "Необходимо собрать минимум 10 осмысленных (`np.radndom.rand()` не подойдет) признаков, при этом:\n",
        "1. 2 должны относиться только к сущности \"пользователь\" (например средний % просмотра фильмов у этой возрастной категории)\n",
        "2. 2 должны относиться только к сущности \"айтем\" (например средний средний % просмотра данного фильма)\n",
        "3. 6 признаков, которые показывают связь пользователя и айтема (например средний % просмотра фильмов с данным актером (айтем) у пользователей с таким же полом (пользователь)).\n",
        "\n",
        "### ВАЖНО!  \n",
        "\n",
        "1. **В датасете есть колонка `watched_prct`. Ее можно использовать для генерации признаков (например сколько пользователь в среднем смотрит фильмы), но нельзя подавать в модель, как отдельную фичу, потому что она напрямую связана с target.**\n",
        "2. **Все признаки должны быть собраны без дата лика, то есть если пользователь посмотрел фильм 10 августа, то признаки мы можем считать только на данных до 9 августа включительно.**\n",
        "\n",
        "\n",
        "### Разбалловка\n",
        "Обучение ранкера будет проходить на `valid_stage_1`, как  раз на которой мы валидировали модели, а тестировать на `test`. Поэтому есть 2 варианта сборки признаков, **реализовать нужно только 1 из них:**\n",
        "1. Для обучения собираем признаки на первый день `valid_stage_1`, а для теста на первый день `test`. Например, если `valid_stage_1` начинается 5 сентября, то все признаки мы можем собирать только по 4 сентября включительно. **(1 балл)**\n",
        "2. Признаки будем собирать честно на каждый день, то есть на 5 сентября собираем с начала до 4, на 6 сентября с начала до 5 и т.д. **(2 балла)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhGi2EDvWZCs"
      },
      "outputs": [],
      "source": [
        "train_df_with_features = # YOUR CODE IS HERE\n",
        "test_df_with_features = # YOUR CODE IS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unxssAkSWZCs"
      },
      "source": [
        "\n",
        "### 3 Этап. Обучение финального ранкера (max 2 балла)\n",
        "Собрав все признаки из этапа 2, добавив скоры моделей из этапа 1 для каждой пары пользователь-айтем (где это возможно), пришло время обучать ранкер. В качестве ранкера можно использовать либо [xgboost](https://xgboost.readthedocs.io/en/stable/) или [catboost](https://catboost.ai/). Обучать можно как `Classfier`, так и `Ranker`, выбираем то, что лучше сработает. Обучение ранкера будет проходить на `valid_stage_1`, как  раз на которой мы валидировали модели, а тестировать на `test`, которую мы до сих пор не трогали.  Заметьте, что у нас в тесте есть холодные пользователи – те, кого не было в train и активные – те, кто был в train. Возможно их стоит обработать по отдельности (а может и нет).  \n",
        "(1 балл)\n",
        "\n",
        "После получения лучшей модели надо посмотреть на важность признаков и [shap values](https://shap.readthedocs.io/en/latest/index.html), чтобы:\n",
        "1. Интерпритировать признаки, которые вы собрали, насколько они полезные\n",
        "2. Проверить наличие ликов – если важность фичи в 100 раз больше, чем у всех остальных, то явно что-то не то  \n",
        "\n",
        "(1 балл)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9156MH9rWZCs"
      },
      "outputs": [],
      "source": [
        "# YOUR FIT PREDICT CODE HERE\n",
        "model.fit()\n",
        "model.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPj0Q0PaWZCs"
      },
      "source": [
        "\n",
        "### 4 Этап. Инференс лучшего ранкера (max 3 балла)\n",
        "\n",
        "Теперь мы хотим построить рекомендации \"на завтра\", для этого нам нужно:\n",
        "\n",
        "1. Обучить модели первого уровня на всех (train+test) данных (0.5 балла)\n",
        "2. Для каждой модели первого уровня для каждого пользователя сгененировать N кандидатов (0.5 балла)\n",
        "3. \"Склеить\" всех кандидатов для каждого пользователя (дубли выкинуть), посчитать скоры от всех моделей (0.5 балла)\n",
        "4. Собрать фичи для ваших кандидатов (теперь можем считать признаки на всех данных) (0.5 балла)\n",
        "5. Проскорить всех кандидатов бустингом и оставить k лучших (0.5 балла)\n",
        "6. Посчитать разнообразие(Diversity) и построить график от Diversity(k) (0.5 балла)\n",
        "\n",
        "\n",
        "Все гиперпараметры (N, k) определяете только Вы!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2I1t1N7WZCt"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}